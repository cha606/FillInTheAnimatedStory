{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mynn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 100\n",
    "W = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the size?\n",
    "#AFTER IMPORT\n",
    "data = \n",
    "print(f\"image size (data.size) \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvertImtoNdArray (image):\n",
    "    return image.flatten().astype(np.float32)-np.mean(image)/np.std(image)\n",
    "#we want to flatten the array so that we can have one input layer. Subtracting by the mean then dividing by the std makes it normalized. The float is for precision by we dont want to much precision, so 32 is used.\n",
    "# Sould be uppercase pi (index)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mynn.activations.relu import relu\n",
    "from mynn.layers.dense import dense\n",
    "\n",
    "class Model:\n",
    "    \n",
    "    def __init__(self, x_in, y_in, numConv = 2, tPool=\" max_pool()\", numDensePairs=1, startingLayerOutput = 500, Loss = CrossEntropyLoss):\n",
    "        '''The initial state of the model\n",
    "        Parameters\n",
    "        ----------\n",
    "        x_in = input layer (int)\n",
    "        y_in = output layer (int)\n",
    "        numConc = number of Convolutions as in times of convoluting input (int)\n",
    "        tPool = type of poolin used\n",
    "        numDensePairs = number of dense layers in a pair of two {I like making it mutable for experimentation} (int)\n",
    "        \n",
    "        Loss = loss function (hopefully function)\n",
    "        '''\n",
    "        assert Loss == L1Loss or Loss == L2Loss  or Loss == CrossEntropyLoss, \"Loss function is unknown!\"\n",
    "        \n",
    "        dReduction = 3*W*L #W and L are defined above These might change for resolutions\n",
    "        \n",
    "        List_of_Layers=[] \n",
    "        initial_layer = dense(dReduction,)\n",
    "        List_of_Layers.append(initial_layer)\n",
    "        for i in numDensePairs:\n",
    "            List_of_Layers.append(dense())\n",
    "    \n",
    "    def __call__(startingpic[3,L,W], endingpic[3,L,W]):\n",
    "        '''Forward Pass\n",
    "        \n",
    "        Parameter\n",
    "        ---------\n",
    "        X is a nd.array, mg.Tensor  x.shape(LayerInput,LayerOutput)\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        Overall output of forward pass (N,1) \n",
    "        '''\n",
    "        assert isinstance(startingpic, ndarray), \"The starting image is not an ndarray\"\n",
    "        assert isinstance(endingpic, ndarray), \"The ending image is not an ndarray\"\n",
    "        \n",
    "        return\n",
    "        \n",
    "    def L1Loss(ytruth, ypred):\n",
    "        '''\n",
    "        L1Loss is the standard absolute value of difference between functions\n",
    "        ytruth is the result we want\n",
    "        ypred is the result that we recieve'''\n",
    "        return np.abs(ytruth- ypred)\n",
    "    def L2Loss(ypred, ytruth):\n",
    "        '''\n",
    "        L2Loss is Euclidean distance between the truth and recieved\n",
    "        ytruth is the result we want\n",
    "        ypred is the result that we recieve'''\n",
    "        return((ytruth-ypred)^2)\n",
    "    def CrossEntropyLoss (ytruth, ypred): #Are there different variants?\n",
    "        '''\n",
    "        CrossEntropyLoss uses the log function for the Loss\n",
    "        ytruth is the result we want\n",
    "        ypred is the result that we recieve'''\n",
    "        \n",
    "        return (-(ytruth*np.log(ypred) + (1-ytruth)*np.log(1-ypred)))\n",
    "    \n",
    "    @property\n",
    "    def property(self):\n",
    "        '''Returns the weights of the model'''\n",
    "        \n",
    "    def accuracy(self, pred):\n",
    "        if isinstance(pred, mg.Tensor):\n",
    "        pred = pred.data\n",
    "    return pred == truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mynn.optimizers.adam import Adam\n",
    "model = Model()   #Might have to change\n",
    "optimizer = Adam(model.parameters, learning_rate =.01, weight_decay =.2) #HYPERPARAMETERS why these values?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
